{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Object_Detection.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMBvIDgYENK7ltkt0eTar7h"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"CGTo5V_wQ9Gh"},"source":["!pip install tensorflow==1.15.0\n","!pip install tensorflow-object-detection-api\n","#!pip install tensorflow-gpu==1.15\n","#!pip install tf_slim\n","#!pip install tf-models-official\n","#!pip install tensorflow-estimator==1.15.*"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sexjqk336o_9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620660038641,"user_tz":-60,"elapsed":2354,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"}},"outputId":"c36b97e6-8d34-480c-c92e-977aa0574349"},"source":["import tensorflow as tf\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.15.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Umimv_rBI4P","executionInfo":{"elapsed":1061,"status":"ok","timestamp":1619467840081,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"},"user_tz":-60},"outputId":"2a8164c2-c63c-448d-fd55-a8843c0f883c"},"source":["\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lrnxO6-T-7ku","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620660061728,"user_tz":-60,"elapsed":21493,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"}},"outputId":"96d0c937-fdea-4869-818f-f4adf4e979f2"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G98tBlgU5-By","executionInfo":{"status":"ok","timestamp":1620038158900,"user_tz":-60,"elapsed":982,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"}},"outputId":"3f5ee382-df02-4389-a399-87ea18321c9c"},"source":["model_dir = '/content/drive/MyDrive/MLiSP2/Object_Detection/Model'\n","#!rm -rf '{model_dir}'\n","#os.makedirs(model_dir, exist_ok=True)\n","!ls -ltra '{model_dir}'/.."],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 75131\n","drwx------ 2 root root     4096 Apr 22 10:37 Test_imgs\n","drwx------ 2 root root     4096 Apr 25 10:48 Model\n","drwx------ 2 root root     4096 Apr 25 15:16 Model_2\n","-rw------- 1 root root 73665307 Apr 26 20:05 train.record\n","-rw------- 1 root root      474 Apr 26 20:05 labelmap.pbtxt\n","-rw------- 1 root root  2028100 Apr 26 20:05 test.record\n","-rw------- 1 root root     4070 Apr 28 18:22 pipeline.config\n","-rw------- 1 root root  1222849 May  3 10:35 Object_Detection.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XNamJzM80Tjn"},"source":["# If you forked the repository, you can replace the link.\n","repo_url = 'https://github.com/dctian/DeepPiCar'\n","\n","# Number of training steps.\n","num_steps = 30000  # 200000\n","#num_steps = 100  # 200000\n","\n","# Number of evaluation steps.\n","num_eval_steps = 50\n","\n","\n","# model configs are from Model Zoo github: \n","# https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models\n","MODELS_CONFIG = {\n","    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz\n","    'ssd_mobilenet_v1_quantized': {\n","        'model_name': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18',\n","        'pipeline_file': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync.config',\n","        'batch_size': 12\n","    },    \n","    'ssd_mobilenet_v2': {\n","        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n","        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n","        'batch_size': 12\n","    },\n","    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\n","    'ssd_mobilenet_v2_quantized': {\n","        'model_name': 'ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03',\n","        'pipeline_file': 'ssd_mobilenet_v2_quantized_300x300_coco.config',\n","        'batch_size': 12\n","    },\n","    'faster_rcnn_inception_v2': {\n","        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n","        'pipeline_file': 'faster_rcnn_inception_v2_pets.config',\n","        'batch_size': 12\n","    },\n","    'rfcn_resnet101': {\n","        'model_name': 'rfcn_resnet101_coco_2018_01_28',\n","        'pipeline_file': 'rfcn_resnet101_pets.config',\n","        'batch_size': 12\n","    }\n","}\n","\n","# Pick the model you want to use\n","# Select a model in `MODELS_CONFIG`.\n","# Note: for Edge TPU, you have to:\n","# 1) start with a pretrained model from model zoo, such as above 4\n","# 2) Must be a quantized model, which reduces the model size significantly\n","selected_model = 'ssd_mobilenet_v2_quantized'\n","\n","# Name of the object detection model to use.\n","MODEL = MODELS_CONFIG[selected_model]['model_name']\n","\n","# Name of the pipline file in tensorflow object detection API.\n","pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n","\n","# Training batch size fits in Colabe's Tesla K80 GPU memory for selected model.\n","batch_size = MODELS_CONFIG[selected_model]['batch_size']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LuvyF2mc0mfv","executionInfo":{"elapsed":1146,"status":"ok","timestamp":1619693282915,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"},"user_tz":-60},"outputId":"dd664e90-df08-4435-9ffc-d53aeb0edcd5"},"source":["import os\n","\n","%cd /content\n","\n","repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n","\n","!git clone {repo_url}\n","%cd {repo_dir_path}\n","\n","print('Pull it so that we have the latest code/data')\n","!git pull"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","fatal: destination path 'DeepPiCar' already exists and is not an empty directory.\n","/content/DeepPiCar\n","Pull it so that we have the latest code/data\n","Already up to date.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q5aGZ5FtCoyq","executionInfo":{"elapsed":637,"status":"ok","timestamp":1619693286496,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"},"user_tz":-60},"outputId":"b7e93647-6bed-4e49-9ad1-54e05bc5b8c0"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["doc  driver  LICENSE  models  README.md\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":476},"id":"Akl_gY2w0uiU","executionInfo":{"status":"error","timestamp":1620425433851,"user_tz":-60,"elapsed":10044,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"}},"outputId":"1118ee92-8c08-4c69-fc2f-8f1b7cc48d3e"},"source":["%cd /content\n","!git clone --quiet https://github.com/tensorflow/models.git\n","\n","!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n","\n","!pip install -q Cython contextlib2 pillow lxml matplotlib\n","\n","!pip install -q pycocotools\n","\n","%cd /content/models/research\n","\n","!protoc object_detection/protos/*.proto --python_out=.\n","\n","import os\n","os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n","\n","!python object_detection/builders/model_builder_test.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","^C\n","^C\n","[Errno 2] No such file or directory: '/content/models/research'\n","/content\n","object_detection/protos/*.proto: No such file or directory\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-f434266baaf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PYTHONPATH'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m':/content/models/research/:/content/models/research/slim/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python object_detection/builders/model_builder_test.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    445\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m   result = _run_command(\n\u001b[0;32m--> 447\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    448\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m           \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m           \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m           close_fds=True)\n\u001b[0m\u001b[1;32m    196\u001b[0m       \u001b[0;31m# The child PTY is only needed by the spawned process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    798\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1503\u001b[0m                 \u001b[0merrpipe_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m                     \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m                     \u001b[0merrpipe_data\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrpipe_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B8fFEP9g0xuV","executionInfo":{"elapsed":8202,"status":"ok","timestamp":1619693299159,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"},"user_tz":-60},"outputId":"2463b2ce-dcd4-4307-a568-c89651187308"},"source":["%cd {repo_dir_path}/models/object_detection\n","\n","# Convert train folder annotation xml files to a single csv file,\n","# generate the `label_map.pbtxt` file to `data/` directory as well.\n","!python code/xml_to_csv.py -i data/images/train -o data/annotations/train_labels.csv -l data/annotations\n","\n","# Convert test folder annotation xml files to a single csv.\n","!python code/xml_to_csv.py -i data/images/test -o data/annotations/test_labels.csv\n","\n","# Generate `train.record`\n","!python code/generate_tfrecord.py --csv_input=data/annotations/train_labels.csv --output_path=data/annotations/train.record --img_path=data/images/train --label_map data/annotations/label_map.pbtxt\n","\n","# Generate `test.record`\n","!python code/generate_tfrecord.py --csv_input=data/annotations/test_labels.csv --output_path=data/annotations/test.record --img_path=data/images/test --label_map data/annotations/label_map.pbtxt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/DeepPiCar/models/object_detection\n","Successfully converted xml to csv.\n","Generate `data/annotations/label_map.pbtxt`\n","Successfully converted xml to csv.\n","WARNING:tensorflow:From code/generate_tfrecord.py:134: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n","\n","WARNING:tensorflow:From code/generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n","\n","W0429 10:48:14.936444 140407930324864 module_wrapper.py:139] From code/generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/utils/label_map_util.py:132: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","W0429 10:48:14.943957 140407930324864 module_wrapper.py:139] From /usr/local/lib/python3.7/dist-packages/object_detection/utils/label_map_util.py:132: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","Successfully created the TFRecords: /content/DeepPiCar/models/object_detection/data/annotations/train.record\n","WARNING:tensorflow:From code/generate_tfrecord.py:134: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n","\n","WARNING:tensorflow:From code/generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n","\n","W0429 10:48:17.975683 140386053134208 module_wrapper.py:139] From code/generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/utils/label_map_util.py:132: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","W0429 10:48:17.981145 140386053134208 module_wrapper.py:139] From /usr/local/lib/python3.7/dist-packages/object_detection/utils/label_map_util.py:132: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","Successfully created the TFRecords: /content/DeepPiCar/models/object_detection/data/annotations/test.record\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KF846be_1MIr"},"source":["test_record_fname = '/content/drive/MyDrive/MLiSP2/Object_Detection/test.record'\n","train_record_fname = '/content/drive/MyDrive/MLiSP2/Object_Detection/train.record'\n","label_map_pbtxt_fname = '/content/drive/MyDrive/MLiSP2/Object_Detection/labelmap.pbtxt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"AhKHGMKm1cUW","executionInfo":{"elapsed":9183,"status":"error","timestamp":1619693303693,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"},"user_tz":-60},"outputId":"7fc66b47-6871-4230-b6ec-063ec3a745d8"},"source":["%cd /content/models/research\n","\n","import os\n","import shutil\n","import glob\n","import urllib.request\n","import tarfile\n","MODEL_FILE = MODEL + '.tar.gz'\n","DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n","DEST_DIR = '/content/models/research/pretrained_model'\n","\n","if not (os.path.exists(MODEL_FILE)):\n","    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n","\n","tar = tarfile.open(MODEL_FILE)\n","tar.extractall()\n","tar.close()\n","\n","os.remove(MODEL_FILE)\n","if (os.path.exists(DEST_DIR)):\n","    shutil.rmtree(DEST_DIR)\n","os.rename(MODEL, DEST_DIR)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: '/content/models/research'\n","/content/DeepPiCar/models/object_detection\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-1a340e977a03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEST_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEST_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEST_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03' -> '/content/models/research/pretrained_model'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YIHMneUx1he2","executionInfo":{"elapsed":1298,"status":"ok","timestamp":1619634131545,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"},"user_tz":-60},"outputId":"afb274ee-849d-4bb4-8e99-0c4ea3f95201"},"source":["!echo {DEST_DIR}\n","!ls -alh {DEST_DIR}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/models/research/pretrained_model\n","total 204M\n","drwx------  2 303230 5000 4.0K Jan  4  2019 .\n","drwxr-xr-x 23 root   root 4.0K Apr 28 18:22 ..\n","-rw-------  1 303230 5000  93M Jan  4  2019 model.ckpt.data-00000-of-00001\n","-rw-------  1 303230 5000  68K Jan  4  2019 model.ckpt.index\n","-rw-------  1 303230 5000  20M Jan  4  2019 model.ckpt.meta\n","-rw-------  1 303230 5000 4.3K Jan  4  2019 pipeline.config\n","-rw-------  1 303230 5000  24M Jan  4  2019 tflite_graph.pb\n","-rw-------  1 303230 5000  68M Jan  4  2019 tflite_graph.pbtxt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Zmy4twS21jnH","executionInfo":{"elapsed":1272,"status":"ok","timestamp":1619634131546,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"},"user_tz":-60},"outputId":"f2ff1e9b-a494-44f5-be61-8b6ac1da58a2"},"source":["fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n","fine_tune_checkpoint"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/models/research/pretrained_model/model.ckpt'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"WXF60nRd1nRN"},"source":["import os\n","pipeline_fname = \"/content/drive/MyDrive/MLiSP2/Object_Detection/pipeline.config\"\n","assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"To1Teby21tNV"},"source":["import re\n","\n","# training pipeline file defines:\n","# - pretrain model path\n","# - the train/test sets\n","# - ID to Label mapping and number of classes\n","# - training batch size\n","# - epochs to trains\n","# - learning rate\n","# - etc\n","\n","# note we just need to use a sample one, and make edits to it.\n","\n","num_classes = 6\n","with open(pipeline_fname) as f:\n","    s = f.read()\n","with open(pipeline_fname, 'w') as f:\n","    \n","    # fine_tune_checkpoint: downloaded pre-trained model checkpoint path\n","    s = re.sub('fine_tune_checkpoint: \".*?\"',\n","               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n","    \n","    # tfrecord files train and test, we created earlier with our training/test sets\n","    s = re.sub(\n","        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n","    s = re.sub(\n","        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n","\n","    # label_map_path: ID to label file\n","    s = re.sub(\n","        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n","\n","    # Set training batch_size.\n","    s = re.sub('batch_size: [0-9]+',\n","               'batch_size: {}'.format(batch_size), s)\n","\n","    # Set training steps, num_steps (Number of epochs to train)\n","    s = re.sub('num_steps: [0-9]+',\n","               'num_steps: {}'.format(num_steps), s)\n","    \n","    # Set number of classes num_classes.\n","    s = re.sub('num_classes: [0-9]+',\n","               'num_classes: {}'.format(num_classes), s)\n","    f.write(s)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xBwVR9bf10NT","executionInfo":{"elapsed":1066,"status":"ok","timestamp":1619693190948,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"},"user_tz":-60},"outputId":"5211b78f-bb9e-4fa6-c85c-4a444f13f480"},"source":["%cd /content/drive/MyDrive/MLiSP2/Object_Detection/\n","!ls\n","!cat {label_map_pbtxt_fname}\n","#%cd /content/models/research"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/MLiSP2/Object_Detection\n","labelmap.pbtxt\tModel_2\t\t\tpipeline.config  test.record\n","Model\t\tObject_Detection.ipynb\tTest_imgs\t train.record\n","item {\n","    name: \"box\",\n","    id: 1,\n","    display_name: \"box\"\n","}\n","item {\n","    name: \"left_turn\",\n","    id: 2,\n","    display_name: \"left_turn\"\n","}\n","item {\n","    name: \"person\",\n","    id: 3,\n","    display_name: \"person\"\n","}\n","item {\n","    name: \"right_turn\",\n","    id: 4,\n","    display_name: \"right_turn\"\n","}\n","item {\n","    name: \"traffic_light_down_green\",\n","    id: 5,\n","    display_name: \"traffic_light_down_green\"\n","}\n","item {\n","    name: \"traffic_light_up_red\",\n","    id: 6,\n","    display_name: \"traffic_light_up_red\"\n","}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bfxQ9xLO17UN","executionInfo":{"status":"ok","timestamp":1619881877987,"user_tz":-60,"elapsed":852,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"}},"outputId":"53ff21ca-e61e-4fb9-a4cd-751bf4b2d89c"},"source":["!cat {pipeline_fname}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cat: {pipeline_fname}: No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jP_7QSsF5qqC","executionInfo":{"elapsed":3993,"status":"ok","timestamp":1619468987144,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"},"user_tz":-60},"outputId":"c68c52cd-a5bf-499c-a063-3aabd6d46a65"},"source":["!pip install lvis"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: lvis in /usr/local/lib/python3.7/dist-packages (0.5.3)\n","Requirement already satisfied: pyparsing>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from lvis) (2.4.7)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from lvis) (1.15.0)\n","Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from lvis) (3.2.2)\n","Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis) (1.3.1)\n","Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.7/dist-packages (from lvis) (1.19.5)\n","Requirement already satisfied: Cython>=0.29.12 in /usr/local/lib/python3.7/dist-packages (from lvis) (0.29.22)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis) (0.10.0)\n","Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis) (4.1.2.30)\n","Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from lvis) (2.8.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lwZTHT5u3jkY"},"source":["# TRAINING\n","num_steps = 30000\n","#SendEmail(\"Colab train started\")\n","!python /content/models/research/object_detection/model_main.py \\\n","    --pipeline_config_path={pipeline_fname} \\\n","    --model_dir='{model_dir}' \\\n","    --alsologtostderr \\\n","    --num_train_steps={num_steps} \\\n","    --num_eval_steps={num_eval_steps}\n","#SendEmail(\"Colab train finished\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y3CbCdk93ej4"},"source":["!ls -ltra '{model_dir}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVUfiyWkJMlg"},"source":["lst = os.listdir(model_dir)\n","# find the last model checkpoint file, i.e. model.ckpt-1000.meta\n","lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n","steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n","last_model = lst[steps.argmax()].replace('.meta', '')\n","\n","last_model_path = os.path.join(model_dir, last_model)\n","print(last_model_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZcQQKnuJOVi"},"source":["!echo creates the frozen inference graph in fine_tune_model\n","# there is an \"Incomplete shape\" message.  but we can safely ignore that. \n","!python /content/models/research/object_detection/export_inference_graph.py \\\n","    --input_type=image_tensor \\\n","    --pipeline_config_path={pipeline_fname} \\\n","    --output_directory='{output_directory}' \\\n","    --trained_checkpoint_prefix='{last_model_path}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dzUE7LW-Jdcr"},"source":["# https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\n","# create the tensorflow lite graph\n","!python /content/models/research/object_detection/export_tflite_ssd_graph.py \\\n","    --pipeline_config_path={pipeline_fname} \\\n","    --trained_checkpoint_prefix='{last_model_path}' \\\n","    --output_directory='{output_directory}' \\\n","    --add_postprocessing_op=true"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ds9rM5uFJfhH"},"source":["!echo \"CONVERTING frozen graph to quantized TF Lite file...\"\n","!tflite_convert \\\n","  --output_file='{output_directory}/road_signs_quantized.tflite' \\\n","  --graph_def_file='{output_directory}/tflite_graph.pb' \\\n","  --inference_type=QUANTIZED_UINT8 \\\n","  --input_arrays='normalized_input_image_tensor' \\\n","  --output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' \\\n","  --mean_values=128 \\\n","  --std_dev_values=128 \\\n","  --input_shapes=1,300,300,3 \\\n","  --change_concat_input_ranges=false \\\n","  --allow_nudging_weights_to_use_fast_gemm_kernel=true \\\n","  --allow_custom_ops"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YS-Te55lH4ks"},"source":["!pip install tensorflow==1.15.0\n","!pip install tensorflow-object-detection-api\n","#!pip install tensorflow-gpu==1.15\n","#!pip install tf_slim\n","#!pip install tf-models-official\n","#!pip install tensorflow-estimator==1.15.*"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nK4k27nmpd3q"},"source":[""]},{"cell_type":"code","metadata":{"id":"GqCCW2ARKa8W"},"source":["#%reset\n","#%matplotlib inline\n","import cv2 as cv\n","import numpy as np\n","from shapely.geometry import Polygon\n","import os\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","from object_detection.utils import ops as utils_ops\n","from object_detection.utils import label_map_util\n","from object_detection.utils import visualization_utils as vis_util\n","from tensorflow.keras.models import load_model\n","import shapely.geometry \n","import descartes\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z6kXAj_8Xw_A"},"source":["print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QYLW5F7NOHqE"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-c_HNwaMtgY"},"source":["test_images = np.asarray(np.load(\"/content/drive/MyDrive/MLiSP2/Holdout_Channels/Holdout_RGB_Not_Resized.pkl\",allow_pickle=True))\n","print(test_images.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TNNTfeapsOT"},"source":["num_classes = 6\n","model_dir = '/content/drive/MyDrive/MLiSP2/Object_Detection/Model'\n","output_directory = '%s/fine_tuned_model' % model_dir\n","pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\") \n","\n","detection_graph = tf.Graph()\n","with detection_graph.as_default():\n","    od_graph_def = tf.GraphDef()\n","    with tf.gfile.GFile(pb_fname, 'rb') as fid:\n","        serialized_graph = fid.read()\n","        od_graph_def.ParseFromString(serialized_graph)\n","        tf.import_graph_def(od_graph_def, name='')\n","\n","label_map_pbtxt_fname = '/content/drive/MyDrive/MLiSP2/Object_Detection/labelmap.pbtxt'\n","label_map = label_map_util.load_labelmap(label_map_pbtxt_fname)\n","categories = label_map_util.convert_label_map_to_categories(\n","label_map, max_num_classes=num_classes, use_display_name=True)\n","category_index = label_map_util.create_category_index(categories)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8uP2evDMKfAo"},"source":["def run_inference_for_single_image(image, graph):\n","    with graph.as_default():\n","        with tf.Session() as sess:\n","            # Get handles to input and output tensors\n","            ops = tf.get_default_graph().get_operations()\n","            all_tensor_names = {\n","                output.name for op in ops for output in op.outputs}\n","            tensor_dict = {}\n","            for key in [\n","                'num_detections', 'detection_boxes', 'detection_scores',\n","                'detection_classes', 'detection_masks'\n","            ]:\n","                tensor_name = key + ':0'\n","                if tensor_name in all_tensor_names:\n","                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n","                        tensor_name)\n","            if 'detection_masks' in tensor_dict:\n","                # The following processing is only for single image\n","                detection_boxes = tf.squeeze(\n","                    tensor_dict['detection_boxes'], [0])\n","                detection_masks = tf.squeeze(\n","                    tensor_dict['detection_masks'], [0])\n","                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n","                real_num_detection = tf.cast(\n","                    tensor_dict['num_detections'][0], tf.int32)\n","                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n","                                           real_num_detection, -1])\n","                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n","                                           real_num_detection, -1, -1])\n","                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n","                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n","                detection_masks_reframed = tf.cast(\n","                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n","                # Follow the convention by adding back the batch dimension\n","                tensor_dict['detection_masks'] = tf.expand_dims(\n","                    detection_masks_reframed, 0)\n","            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n","\n","            # Run inference\n","            output_dict = sess.run(tensor_dict,\n","                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n","\n","            # all outputs are float32 numpy arrays, so convert types as appropriate\n","            output_dict['num_detections'] = int(\n","                output_dict['num_detections'][0])\n","            output_dict['detection_classes'] = output_dict[\n","                'detection_classes'][0].astype(np.uint8)\n","            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n","            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n","            if 'detection_masks' in output_dict:\n","                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n","    return output_dict\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9gfWQzKpL33I"},"source":["#!pip install keras==2.3.0\n","%cd /content/drive/MyDrive/MLiSP2/\n","model = load_model(\"Angle_Model.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IE-6qd4TcNwq"},"source":["def placehold(coordinates, object_detect):\n","  xmax, xmin, ymax, ymin = object_detect\n","  detaction_box = Polygon([[xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax]])\n","  region_of_interest = Polygon([[coordinates[0], coordinates[1] ], [coordinates[2], coordinates[3]], [coordinates[4], coordinates[5]]])\n","  find_intersection = detaction_box.intersection(region_of_interest)\n","  overlap = find_intersection.area\n","  return overlap, find_intersection"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7wPHbFnBWTHe"},"source":["def turn(turn_cord, direction, multiplier=0.008):\n","  ymax, ymin = turn_cord[0]\n","  difference = ymax - ymin\n","\n","  if direction == \"left\":\n","    angle = 0.5 - (difference * multiplier)\n","    angle_amount = max(0, angle)\n","\n","  elif direction == \"right\":\n","    angle = 0.5 + (difference * multiplier)\n","    angle_amount = min(1, angle)\n","\n","  return abs(angle_amount)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kMvwrfr4-g22"},"source":["def a_turn_f(turn_amount):\n","\n","  if turn_amount > 0.66 and turn_amount < 0.78: #right\n","    #number = turn_amount - 0.5\n","    x1 = 140  #Where x1 is the bottom left x value in the triangle\n","    y1 = 240\n","\n","    x2 = 320  #Where x2 is the middle x value in the triangle\n","    y2 = 80 # 120\n","\n","    x3 = 320\n","    y3 = 240\n","  elif turn_amount >= 0.78:\n","    x1 = 120  #Where x1 is the bottom left x value in the triangle\n","    y1 = 240\n","\n","    x2 = 320  #Where x2 is the middle x value in the triangle\n","    y2 = 180 # 120\n","\n","    x3 = 320\n","    y3 = 240\n","\n","  elif turn_amount <= 0.34: #left\n","    x1 = 0  #Where x1 is the bottom left x value in the triangle\n","    y1 = 240\n","\n","    x2 = 0  #Where x2 is the middle x value in the triangle\n","    y2 = 120 # 120\n","\n","    x3 = 160\n","    y3 = 240\n","  \n","  else:\n","    x1 = 0\n","    y1 = 240\n","\n","    x2 = 160\n","    y2 = 60\n","\n","    x3 = 320\n","    y3 = 240\n","\n","  return [x1, y1, x2, y2, x3, y3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LoBnZm5xv6VY"},"source":["def predict_angle(model):\n","      img = cv.resize(test_images[n], (80, 60))  \n","      img_expanded = np.expand_dims(img, axis=0) \n","      img_expanded = img_expanded / 255. \n","      pred = model.predict(img_expanded)[0][0]\n","      return pred "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Cl7LfQOKlHz"},"source":["%matplotlib inline\n","box_loc, person_loc, traffic_light_up_red_loc, speed, angle, left_turn_loc, right_turn_loc = [], [], [], [], [], [], []\n","classes = [\"box\",\n","           \"left_turn\", \n","           \"person\", \n","           \"right_turn\",\n","           \"traffic_light_down_green\",\n","           \"traffic_light_up_red\"]\n","\n","object_threshold = 0.5\n","person_threshold = [76, 64] \n","box_threshold = 60 \n","traffic_light_red_threshold = 60  \n","\n","for i in range(test_images.shape[0]):\n","\n","    # Actual detection.\n","    output_dict = run_inference_for_single_image(test_images[i], detection_graph)\n","\n","    # Visualization of the results of a detection.\n","    vis_util.visualize_boxes_and_labels_on_image_array(\n","        test_images[i],\n","        output_dict['detection_boxes'],\n","        output_dict['detection_classes'],\n","        output_dict['detection_scores'],\n","        category_index,\n","        min_score_thresh=object_threshold,\n","        instance_masks=output_dict.get('detection_masks'),\n","        use_normalized_coordinates=True,\n","        line_thickness=2)\n","\n","    for ii in range(len(output_dict['detection_scores'])):\n","      if output_dict['detection_scores'][ii] >= object_threshold:\n","\n","        idx = output_dict['detection_classes'][ii] - 1\n","        current_class = classes[idx]\n","        ymin, xmin, ymax, xmax = output_dict['detection_boxes'][ii]\n","        width, height = 320, 240\n","        ymin, xmin, ymax, xmax = ymin * height, xmin * width, ymax * height, xmax * width\n","        if current_class == \"person\":\n","          person_loc.append([xmax, xmin, ymax, ymin])\n","        elif current_class == \"box\":\n","          box_loc.append([xmax, xmin, ymax, ymin])\n","        elif current_class == \"traffic_light_up_red\":\n","          traffic_light_up_red_loc.append([ymax, ymin])\n","        elif current_class == \"left_turn\":\n","          left_turn_loc.append([ymax, ymin])\n","        elif current_class == \"right_turn\":\n","          right_turn_loc.append([ymax, ymin])\n","\n","\n","        if left_turn_loc and len(angle) != (i + 1):\n","          angle.append(turn(left_turn_loc, direction = \"left\"))\n","        elif right_turn_loc and len(angle) != (i + 1):\n","          angle.append(turn(right_turn_loc, direction = \"right\"))\n","          \n","        if person_loc:\n","            if (person_loc[-1][0] - person_loc[-1][1] > person_threshold[0]) or (person_loc[-1][2] - person_loc[-1][3] > person_threshold[1]):\n","              #area_triangle = 9600*(2*angle[0] + 1)*(1 - angle[-1])\n","              #threshold_for_triangle = area_triangle * 300 / 9600\n","              if len(angle) != (i + 1):\n","                angle.append(predict_angle(model))\n","\n","              coordinates = a_turn_f(0.5)\n","              #print(person_loc[iii])\n","              area, _ = placehold(coordinates, person_loc[-1])\n","\n","              #print(f\"Area of trig: {area_triangle} | Threshold of trig: {threshold_for_triangle}\")\n","              #print(f\"Overlap Area: {area}\")\n","\n","              if area > 0 and len(speed) != (i + 1):\n","                speed.append(0)\n","                \n","      \n","        if box_loc and len(speed) != (i + 1):\n","          #print(f\"xmin: {box_loc[-1][1]}\\nxmax: {box_loc[-1][0]}\\nymin: {box_loc[-1][-1]}\\nymax: {box_loc[-1[2]}\")\n","          if (box_loc[-1][2] - box_loc[-1][3] > box_threshold):\n","            if len(angle) != (i + 1):\n","              angle.append(predict_angle(model))\n","              \n","            #area_triangle = 9600*(2*angle[-1] + 1)*(1 - angle[-1])\n","            #threshold_for_triangle = area_triangle * 300 / 9600\n","            coordinates = a_turn_f(angle[-1])\n","            #print(box_loc[-1])\n","            area, _ = placehold(coordinates, box_loc[-1])\n","            #print(f\"area of trig: {area_triangle} | threshold of trig: {threshold_for_triangle}\")\n","            #print(f\"Overlap Area: {area}\")\n","            if area > 0 and len(speed) != (i + 1):\n","              speed.append(0)\n","              \n","\n","        if traffic_light_up_red_loc and len(speed) != (i + 1):\n","          if (traffic_light_up_red_loc[0][0] - traffic_light_up_red_loc[0][1]  > traffic_light_red_threshold):\n","              speed.append(0)\n","              \n","      \n","\n","\n","\n","      traffic_light_up_red_loc.clear()\n","      box_loc.clear()\n","      person_loc.clear()\n","      left_turn_loc.clear()\n","      traffic_light_up_red_loc.clear()\n","      right_turn_loc.clear()\n","\n","    if len(angle) != (i + 1):\n","      angle.append(predict_angle(model))\n","    if len(speed) != (i + 1):\n","        speed.append(1)\n","\n","    print(f\"Speed: {speed} \\nAngle: {angle}\")\n","   \n","    if len(speed) == (i + 1) and len(angle) == (i+1):\n","      break\n","\n","\n"," \n","\n","\n","        #if i % 100 == 0:\n","          #print(f\"Progress: {round((i + 1) / test_images.shape[0], 2)}%\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oHK6l32LJf3L"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qMLP3Bdtee7D"},"source":["%cd /content/drive/MyDrive/MLiSP2\n","speed_array = np.asarray(speed, dtype=int).reshape(-1, 1)\n","angle_array = np.asarray(angle, dtype=float).reshape(-1, 1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMCUhRpfXLB1","executionInfo":{"status":"ok","timestamp":1620295688567,"user_tz":-60,"elapsed":638,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"}},"outputId":"1f1f6dff-8460-4f53-a9d2-09c52936114b"},"source":["import pandas as pd\n","\n","columns = [\"image_id\", \"angle\", \"speed\"]\n","\n","image_id = np.asarray(range(1, test_images.shape[0] + 1)).reshape(-1, 1).astype(int)\n","\n","concat_columns = np.concatenate((image_id, angle_array, speed_array), axis=1)\n","results = pd.DataFrame(concat_columns, columns=columns)\n","\n","results[\"image_id\"] = results[\"image_id\"].astype(int)\n","results[\"speed\"] = results[\"speed\"].astype(int)\n","print(results)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["      image_id     angle  speed\n","0            1  0.562249      0\n","1            2  0.799274      1\n","2            3  0.328672      1\n","3            4  0.255977      1\n","4            5  0.075842      1\n","...        ...       ...    ...\n","1015      1016  0.497246      1\n","1016      1017  0.666625      0\n","1017      1018  0.461630      1\n","1018      1019  0.000000      1\n","1019      1020  0.244005      1\n","\n","[1020 rows x 3 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kHvNnFcEiMLR","executionInfo":{"status":"ok","timestamp":1620295691265,"user_tz":-60,"elapsed":422,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"}},"outputId":"3e727b6b-b7ca-40b8-95f2-adb59818d9dd"},"source":["results[\"speed\"].value_counts()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    564\n","0    456\n","Name: speed, dtype: int64"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"1OSUCUv2WqZ1"},"source":["results.to_csv(\"/content/drive/MyDrive/MLiSP2/holdout_submission.csv\", header=True, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gPwKFLKa38ty","executionInfo":{"status":"ok","timestamp":1620426655992,"user_tz":-60,"elapsed":6857,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"}},"outputId":"916e3091-5b30-4fe2-858f-f0837c19724e"},"source":["!python /content/models/research/object_detection/export_inference_graph.py \\\n","    --input_type=image_tensor \\\n","    --pipeline_config_path={pipeline_fname} \\\n","    --output_directory='{output_directory}' \\\n","    --trained_checkpoint_prefix='{last_model_path}'\n","\n","# create the tensorflow lite graph\n","!python /content/models/research/object_detection/export_tflite_ssd_graph.py \\\n","    --pipeline_config_path={pipeline_fname} \\\n","    --trained_checkpoint_prefix='{last_model_path}' \\\n","    --output_directory='{output_directory}' \\\n","    --add_postprocessing_op=true\n","\n","# CONVERTING frozen graph to quantized TF Lite file...\n","!tflite_convert \\\n","  --output_file='{output_directory}/road_signs_quantized.tflite' \\\n","  --graph_def_file='{output_directory}/tflite_graph.pb' \\\n","  --inference_type=QUANTIZED_UINT8 \\\n","  --input_arrays='normalized_input_image_tensor' \\\n","  --output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' \\\n","  --mean_values=128 \\\n","  --std_dev_values=128 \\\n","  --input_shapes=1,300,300,3 \\\n","  --change_concat_input_ranges=false \\\n","  --allow_nudging_weights_to_use_fast_gemm_kernel=true \\\n","  --allow_custom_ops"],"execution_count":null,"outputs":[{"output_type":"stream","text":["python3: can't open file '/content/models/research/object_detection/export_inference_graph.py': [Errno 2] No such file or directory\n","python3: can't open file '/content/models/research/object_detection/export_tflite_ssd_graph.py': [Errno 2] No such file or directory\n","2021-05-07 22:30:51.284043: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-05-07 22:30:51.295520: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","2021-05-07 22:30:51.295590: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a9530f3e1da4): /proc/driver/nvidia/version does not exist\n","2021-05-07 22:30:51.296120: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2021-05-07 22:30:51.303868: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n","2021-05-07 22:30:51.304160: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a0d320ed80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-05-07 22:30:51.304199: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/tflite_convert\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/python/tflite_convert.py\", line 515, in main\n","    app.run(main=run_main, argv=sys.argv[:1])\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n","    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n","  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 303, in run\n","    _run_main(main, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\n","    sys.exit(main(argv))\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/python/tflite_convert.py\", line 511, in run_main\n","    _convert_tf1_model(tflite_flags)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/python/tflite_convert.py\", line 199, in _convert_tf1_model\n","    output_data = converter.convert()\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/python/lite.py\", line 989, in convert\n","    **converter_kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/python/convert.py\", line 412, in toco_convert_graph_def\n","    enable_mlir_converter=enable_mlir_converter)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/python/convert.py\", line 200, in toco_convert_protos\n","    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\n","tensorflow.lite.python.convert.ConverterError: See console for info.\n","2021-05-07 22:30:54.166509: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TFLite_Detection_PostProcess\n","2021-05-07 22:30:54.172130: F tensorflow/lite/toco/tooling_util.cc:1669] Check failed: input_array_dims[i] == input_array_proto.shape().dims(i) (300 vs. 240)\n","Fatal Python error: Aborted\n","\n","Current thread 0x00007f35587c9780 (most recent call first):\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52 in execute\n","  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251 in _run_main\n","  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 303 in run\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/platform/app.py\", line 40 in run\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89 in main\n","  File \"/usr/local/bin/toco_from_protos\", line 8 in <module>\n","Aborted (core dumped)\n","\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nkQSdmdU6Yxq","executionInfo":{"status":"ok","timestamp":1620425704355,"user_tz":-60,"elapsed":682,"user":{"displayName":"OJ L","photoUrl":"","userId":"12976686999064123497"}},"outputId":"73251537-8820-4793-e729-e1afde20a75f"},"source":["# download this file from google drive.\n","#%cd /content/gdrive/MyDrive/MLiSP2/Object_Detection/Model_2/fine_tuned_model/\n","\n","!ls -lt '/content/drive/MyDrive/MLiSP2/Object_Detection/Model/fine_tuned_model/road_signs_quantized.tflite'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-rw------- 1 root root 4793304 May  7 22:15 /content/drive/MyDrive/MLiSP2/Object_Detection/Model/fine_tuned_model/road_signs_quantized.tflite\n"],"name":"stdout"}]}]}